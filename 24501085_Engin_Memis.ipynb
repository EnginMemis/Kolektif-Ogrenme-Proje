{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjhdtrNBu8ZX"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYPUI6XIvza6"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3x647r6Iv0Sy"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8-zjpCNv2h6"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3hxsELHv8Ji"
      },
      "outputs": [],
      "source": [
        "#Veri Setini Yükleme\n",
        "dataset = load_dataset(\"Metin/WikiRAG-TR\", split=\"train[:1000]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX8lE9bDv92q"
      },
      "outputs": [],
      "source": [
        "original_model_name = \"intfloat/multilingual-e5-base\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnHF8Lwwv_yq"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(original_model_name)\n",
        "model = AutoModel.from_pretrained(original_model_name).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7i6eIz0rwD1C"
      },
      "outputs": [],
      "source": [
        "#Embedding Alma Fonksiyonu\n",
        "def get_embeddings(texts, tokenizer, model):\n",
        "    embeddings = []\n",
        "    print(len(texts))\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        embeddings.append(outputs.last_hidden_state[:, 0, :].cpu().numpy()[0])\n",
        "    return np.array(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0LnSpvrwGZC"
      },
      "outputs": [],
      "source": [
        "# WikiRag Verisetindeki split pointslere göre chunk'ları çıkarma işlemi\n",
        "contexts = []\n",
        "questions = []\n",
        "labels = []\n",
        "faiss_indices = []\n",
        "\n",
        "current_index = 0\n",
        "for example in dataset:\n",
        "    questions.append(example['question'])\n",
        "    correct_intro_idx = example['correct_intro_idx']\n",
        "    split_points = list(map(int, example['ctx_split_points'][1:-1].split(',')))\n",
        "    split_points = [0] + split_points\n",
        "\n",
        "    for idx in range(len(split_points) - 1):\n",
        "        context_chunk = example['context'][split_points[idx]:split_points[idx + 1]]\n",
        "        contexts.append(context_chunk)\n",
        "        if idx == correct_intro_idx:\n",
        "            labels.append(current_index)\n",
        "        current_index += 1\n",
        "\n",
        "print(\"Total Chunk: \", len(contexts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ozPQwELwJxK"
      },
      "outputs": [],
      "source": [
        "context_embeddings = get_embeddings(contexts, tokenizer, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30onwy-twafi"
      },
      "outputs": [],
      "source": [
        "question_embeddings = get_embeddings(questions, tokenizer, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8Wv66iYw57E"
      },
      "outputs": [],
      "source": [
        "# Vector Database'i oluşturma\n",
        "embedding_dim = context_embeddings.shape[1]\n",
        "faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
        "faiss_index.add(context_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_5OcCzwxeeV"
      },
      "outputs": [],
      "source": [
        "retrieved_top_1 = []\n",
        "retrieved_top_5 = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU89DqLFxUW8"
      },
      "outputs": [],
      "source": [
        "# Vector Database'inde yakın cevapları arama işlemi\n",
        "for idx, q_emb in enumerate(question_embeddings):\n",
        "    distances, indices = faiss_index.search(np.expand_dims(q_emb, axis=0), k=5)\n",
        "    correct_idx = labels[idx]\n",
        "    retrieved_top_1.append(correct_idx in indices[0][:1])\n",
        "    retrieved_top_5.append(correct_idx in indices[0][:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erkXG_XYxc4c"
      },
      "outputs": [],
      "source": [
        "accuracy_top_1 = accuracy_score(retrieved_top_1, [1] * len(retrieved_top_1))\n",
        "accuracy_top_5 = accuracy_score(retrieved_top_5, [1] * len(retrieved_top_5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nm-Vsp6YxgRc"
      },
      "outputs": [],
      "source": [
        "print(f\"Original Model Top-1 Accuracy: {accuracy_top_1:.2f}\")\n",
        "print(f\"Original Model Top-5 Accuracy: {accuracy_top_5:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwaqVmJu5ecp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJxENQM45d7S"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pf3pxlyj08k6"
      },
      "outputs": [],
      "source": [
        "train_dataset = load_dataset(\"WhiteAngelss/Turkce-Duygu-Analizi-Dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezZiv8U71Urq"
      },
      "outputs": [],
      "source": [
        "train_subset = train_dataset['train'].train_test_split(test_size=0.01)['test']\n",
        "test_subset = train_dataset['test'].train_test_split(test_size=0.01)['test']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "label_counts = Counter(train_subset['label'])\n",
        "\n",
        "# Counter sonuçlarını ayır\n",
        "label_names = list(label_counts.keys())\n",
        "label_values = list(label_counts.values())\n",
        "\n",
        "# Bar grafiği çizdir\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(label_names, label_values)\n",
        "plt.xlabel('Labels')\n",
        "plt.ylabel('Counts')\n",
        "plt.title('Train Dataset')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NCxShx6ficFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_counts = Counter(test_subset['label'])\n",
        "\n",
        "# Counter sonuçlarını ayır\n",
        "label_names = list(label_counts.keys())\n",
        "label_values = list(label_counts.values())\n",
        "\n",
        "# Bar grafiği çizdir\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(label_names, label_values)\n",
        "plt.xlabel('Labels')\n",
        "plt.ylabel('Counts')\n",
        "plt.title('Test Dataset')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7Wm7SOi7incW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbj-msLe1ZKy"
      },
      "outputs": [],
      "source": [
        "len(train_subset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fc1N6xds1nWb"
      },
      "outputs": [],
      "source": [
        "train_subset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLpptU3X1rRL"
      },
      "outputs": [],
      "source": [
        "test_subset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ad4hPIxl11GT"
      },
      "outputs": [],
      "source": [
        "small_dataset = {\n",
        "    'train': train_subset,\n",
        "    'test': test_subset\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yf-JU_SN17bD"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGK_sAl91-gD"
      },
      "outputs": [],
      "source": [
        "model_name = \"intfloat/multilingual-e5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJw-HH-m2F88"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRMzlPuw4RNw"
      },
      "outputs": [],
      "source": [
        "def preprocess_labels(example):\n",
        "    example['label'] = label_mapping[example['label']]\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJJRTLc34Fbn"
      },
      "outputs": [],
      "source": [
        "label_mapping = {\"Positive\": 0, \"Negative\": 1, \"Notr\": 2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdslmbXG2J5s"
      },
      "outputs": [],
      "source": [
        "# Veri Setini eğitime uygun hale getirme işlemi\n",
        "encoded_train = train_subset.map(preprocess_function, batched=True)\n",
        "encoded_test = test_subset.map(preprocess_function, batched=True)\n",
        "\n",
        "encoded_train = encoded_train.map(preprocess_labels)\n",
        "encoded_test = encoded_test.map(preprocess_labels)\n",
        "\n",
        "encoded_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "encoded_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuZIs1n92NUE"
      },
      "outputs": [],
      "source": [
        "encoded_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDMt9eKE2god"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzOPviSp2U5c"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_dataloader = DataLoader(encoded_train, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(encoded_test, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig0MMD7n2g5c"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Optimizasyon fonksiyonu\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Cihaz ayarı\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    progress_bar = tqdm(train_dataloader, desc=\"Training\", leave=True)\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Verileri modele gönder\n",
        "        inputs = {key: batch[key].to(device) for key in ['input_ids', 'attention_mask']}\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        # Model ileri geçişi\n",
        "        outputs = model(**inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Geriye yayılım\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # İlerleme çubuğunda loss'u güncelle\n",
        "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} tamamlandı. Ortalama Loss: {epoch_loss / len(train_dataloader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQhelKwE2kAs"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model.eval()\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        inputs = {key: batch[key].to(device) for key in ['input_ids', 'attention_mask']}\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        preds = torch.argmax(outputs.logits, axis=1)\n",
        "\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "print(classification_report(true_labels, predictions, target_names=['Positive', 'Negative', 'Notr']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im6S9INB5X0R"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"e5-turkish-base_small\")\n",
        "tokenizer.save_pretrained(\"e5-turkish-base_small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HC4GL_fV5bOR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyUY-Ja18iue"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEJFdSD18inG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvxVx2S28i-D"
      },
      "outputs": [],
      "source": [
        "original_model_name = \"e5-turkish-base_small\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMArnDXz8i-D"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(original_model_name)\n",
        "model = AutoModel.from_pretrained(original_model_name).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TV_cIl28i-D"
      },
      "outputs": [],
      "source": [
        "contexts = []\n",
        "questions = []\n",
        "labels = []\n",
        "faiss_indices = []\n",
        "\n",
        "current_index = 0\n",
        "for example in dataset:\n",
        "    questions.append(example['question'])\n",
        "    correct_intro_idx = example['correct_intro_idx']\n",
        "    split_points = list(map(int, example['ctx_split_points'][1:-1].split(',')))\n",
        "    split_points = [0] + split_points\n",
        "    for idx in range(len(split_points) - 1):\n",
        "        context_chunk = example['context'][split_points[idx]:split_points[idx + 1]]\n",
        "        contexts.append(context_chunk)\n",
        "        if idx == correct_intro_idx:\n",
        "            labels.append(current_index)\n",
        "        current_index += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvwaTCUx8i-D"
      },
      "outputs": [],
      "source": [
        "context_embeddings = get_embeddings(contexts, tokenizer, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sblqzfM8i-D"
      },
      "outputs": [],
      "source": [
        "question_embeddings = get_embeddings(questions, tokenizer, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "653juA7O8i-D"
      },
      "outputs": [],
      "source": [
        "embedding_dim = context_embeddings.shape[1]\n",
        "faiss_new_index = faiss.IndexFlatL2(embedding_dim)\n",
        "faiss_new_index.add(context_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_E_N4ft8i-D"
      },
      "outputs": [],
      "source": [
        "retrieved_top_1 = []\n",
        "retrieved_top_5 = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b3m9BSs8i-D"
      },
      "outputs": [],
      "source": [
        "for idx, q_emb in enumerate(question_embeddings):\n",
        "    distances, indices = faiss_new_index.search(np.expand_dims(q_emb, axis=0), k=5)\n",
        "    correct_idx = labels[idx]\n",
        "    print(indices, correct_idx)\n",
        "    retrieved_top_1.append(correct_idx in indices[0][:1])\n",
        "    retrieved_top_5.append(correct_idx in indices[0][:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miVZDiW98i-D"
      },
      "outputs": [],
      "source": [
        "accuracy_top_1 = accuracy_score(retrieved_top_1, [1] * len(retrieved_top_1))\n",
        "accuracy_top_5 = accuracy_score(retrieved_top_5, [1] * len(retrieved_top_5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8nmuyX78i-D"
      },
      "outputs": [],
      "source": [
        "print(f\"Fine-Tuned Model Top-1 Accuracy: {accuracy_top_1:.2f}\")\n",
        "print(f\"Fine-Tuned Model Top-5 Accuracy: {accuracy_top_5:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqIf6tspoI2j"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}